{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6rpmRwxhk8k",
        "outputId": "28e8bf01-40eb-48cd-862a-a6aba21c405e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camel-tools\n",
            "  Downloading camel_tools-1.5.2-py3-none-any.whl (124 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/124.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.16.0)\n",
            "Collecting docopt (from camel-tools)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from camel-tools) (5.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.11.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from camel-tools) (1.2.2)\n",
            "Collecting dill (from camel-tools)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from camel-tools) (4.35.2)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from camel-tools) (2.31.0)\n",
            "Collecting emoji (from camel-tools)\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyrsistent (from camel-tools)\n",
            "  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from camel-tools) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from camel-tools) (4.66.1)\n",
            "Collecting muddler (from camel-tools)\n",
            "  Downloading muddler-0.1.3-py3-none-any.whl (16 kB)\n",
            "Collecting camel-kenlm>=2023.3.17.2 (from camel-tools)\n",
            "  Downloading camel-kenlm-2023.3.17.2.tar.gz (426 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.6/426.6 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->camel-tools) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->camel-tools) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->camel-tools) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->camel-tools) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->camel-tools) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->camel-tools) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->camel-tools) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.2->camel-tools) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.2->camel-tools) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.2->camel-tools) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.2->camel-tools) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.2->camel-tools) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.2->camel-tools) (0.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->camel-tools) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->camel-tools) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->camel-tools) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->camel-tools) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->camel-tools) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->camel-tools) (1.3.0)\n",
            "Building wheels for collected packages: camel-kenlm, docopt\n",
            "  Building wheel for camel-kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2023.3.17.2-cp310-cp310-linux_x86_64.whl size=3453097 sha256=e8b9606c896c276eda5adb65e69e5a8fb460659fcad49bc2d9ec63f807d96606\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/c5/32/09633c3b70fdfc470b2fb912bd9e90d8d6814df68c794dcaa6\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=44e1fb1b139dedd69ef8c826e7aa1f16d61be02c2a8ba22b0dc3a7cf68f6d627\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built camel-kenlm docopt\n",
            "Installing collected packages: docopt, camel-kenlm, pyrsistent, muddler, emoji, dill, camel-tools\n",
            "Successfully installed camel-kenlm-2023.3.17.2 camel-tools-1.5.2 dill-0.3.7 docopt-0.6.2 emoji-2.8.0 muddler-0.1.3 pyrsistent-0.20.0\n",
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "%pip install camel-tools\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "os.environ['CAMELTOOLS_DATA'] = '/gdrive/MyDrive/camel_tools'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D3RRPuO7mQ5"
      },
      "outputs": [],
      "source": [
        "from camel_tools.utils.normalize import normalize_unicode\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize\n",
        "from camel_tools.disambig.mle import MLEDisambiguator\n",
        "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "from nltk import FreqDist\n",
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('//gdrive/MyDrive/AI Project - Classifying Machine-Translated And Human-Translated Arabic Text/New dataset with test/ar_en_dev_filtered.csv')\n",
        "df2 = pd.read_csv('//gdrive/MyDrive/AI Project - Classifying Machine-Translated And Human-Translated Arabic Text/New dataset with test/ar_en_tune_filtered.csv')\n",
        "df3 = pd.read_csv('//gdrive/MyDrive/AI Project - Classifying Machine-Translated And Human-Translated Arabic Text/New dataset with test/ar_fr_dev_filtered.csv')\n",
        "df4 = pd.read_csv('//gdrive/MyDrive/AI Project - Classifying Machine-Translated And Human-Translated Arabic Text/New dataset with test/ar_fr_tune_filtered.csv')\n"
      ],
      "metadata": {
        "id": "VGm8NUxu4OkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKkqZDftmRPK"
      },
      "outputs": [],
      "source": [
        "def melt_and_label(df, mt_column, ht_column):\n",
        "    mt_melted = pd.melt(df, value_vars=[mt_column], var_name='column', value_name='text')\n",
        "    mt_melted['labels'] = 'MT'\n",
        "\n",
        "    ht_melted = pd.melt(df, value_vars=[ht_column], var_name='column', value_name='text')\n",
        "    ht_melted['labels'] = 'HT'\n",
        "\n",
        "    melted_df = pd.concat([mt_melted, ht_melted], ignore_index=True)\n",
        "\n",
        "    return melted_df\n",
        "\n",
        "df1_melted = melt_and_label(df1, 'ar_mt', 'ar_ht')\n",
        "df2_melted = melt_and_label(df2, 'ar_mt', 'ar_ht')\n",
        "df3_melted = melt_and_label(df3, 'ar_mt', 'ar_ht')\n",
        "df4_melted = melt_and_label(df4, 'ar_mt', 'ar_ht')\n",
        "\n",
        "concatenated_df = pd.concat([df1_melted, df2_melted, df3_melted, df4_melted], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df1 = pd.read_csv('/gdrive/MyDrive/AI Project - Classifying Machine-Translated And Human-Translated Arabic Text/New dataset with test/ar_fr_test.csv')\n",
        "test_df2 = pd.read_csv('/gdrive/MyDrive/AI Project - Classifying Machine-Translated And Human-Translated Arabic Text/New dataset with test/ar_en_test.csv')"
      ],
      "metadata": {
        "id": "7lIp6E7o9twW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df1_melted = melt_and_label(test_df1, 'ar_mt', 'ar_ht')\n",
        "test_df2_melted = melt_and_label(test_df2, 'ar_mt', 'ar_ht')\n",
        "\n",
        "test_df = pd.concat([test_df1_melted, test_df2_melted], ignore_index=True)"
      ],
      "metadata": {
        "id": "PHlj12nD9q7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylMJMoLaCIWV"
      },
      "outputs": [],
      "source": [
        "mle = MLEDisambiguator.pretrained()\n",
        "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)\n",
        "\n",
        "def preprocess_text(text):\n",
        "   if pd.isna(text):  #NaN cases\n",
        "        return  str(text)\n",
        "\n",
        "\n",
        "   sent_norm = normalize_unicode(text)\n",
        "   sent_dediac = dediac_ar(sent_norm)\n",
        "   sent_split = sent_dediac.split()\n",
        "   tokens = tokenizer.tokenize(sent_split)\n",
        "   return tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_sent(text):\n",
        "   if pd.isna(text):  #NaN cases\n",
        "        return  str(text)\n",
        "\n",
        "   sent_norm = normalize_unicode(text)\n",
        "   sent_split = sent_norm.split()\n",
        "   tokens = tokenizer.tokenize(sent_split)\n",
        "   return tokens"
      ],
      "metadata": {
        "id": "ecktZ6fkTH06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(sentence):\n",
        "    if isinstance(sentence, str):\n",
        "        return len(sentence.split())\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "concatenated_df['word_count'] = concatenated_df['text'].apply(count_words)\n",
        "\n",
        "print(concatenated_df[['text', 'word_count']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8uegRFBVZlo",
        "outputId": "e49a76a3-137a-44a3-b1b7-3fa20d7a742c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  word_count\n",
            "0      توجيه المجلس الأول بشأن وضع قواعد مشتركة معينة...          18\n",
            "1                      مع مراعاة رأي البرلمان الأوروبي ؛           6\n",
            "2                                 لقد اعتمد هذا التوجيه:           4\n",
            "3      2. يجب إعفاء أنواع النقل المدرجة في الملحق الأ...          15\n",
            "4                                                القسم 2           2\n",
            "...                                                  ...         ...\n",
            "25285                                    من 23 شباط 2004           4\n",
            "25286  مع مراعاة الطلب الذي أدلت به النمسا يوم 30 حزي...          11\n",
            "25287  (3) التدابير المنصوص عليها في هذا النظام هي وف...          22\n",
            "25288  1. تم منح الاستثناءات التالية من أحكام نظام (C...          12\n",
            "25289  ج) تمنح لوكسمبورغ استثناءات فيما يتعلق بتقديم ...          22\n",
            "\n",
            "[25290 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_count = 0\n",
        "\n",
        "for count in concatenated_df['word_count']:\n",
        "    total_count += count\n",
        "\n",
        "avg = total_count / len(concatenated_df['word_count'])\n",
        "\n",
        "print(f\"Average word count: {avg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkSGfKCMWIHM",
        "outputId": "ef26d2fb-4340-4aaf-e604-489c1d09e162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average word count: 27.27105575326216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKjjAeLIq0NL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec6b55c-382e-49ae-f127-ca1b997343f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Trigrams for MT:\n",
            "[(('ال+', 'دول', 'ال+'), 1846), (('+ها', 'في', 'ال+'), 1817), (('دول', 'ال+', 'أعضاء'), 1792), (('في', 'ال+', 'مادة'), 1673), (('ال+', 'منصوص', 'على'), 1436), (('وفقا', 'ل+', 'ال+'), 1287), (('ال+', 'مشار', 'إلى'), 1262), (('مشار', 'إلى', '+ها'), 996), (('منصوص', 'على', '+ها'), 990), (('على', '+ها', 'في'), 972)]\n",
            "Top 10 Trigrams for HT:\n",
            "[(('ال+', 'دول', 'ال+'), 1877), (('دول', 'ال+', 'أعضاء'), 1842), (('+ها', 'في', 'ال+'), 1380), (('في', 'ال+', 'مادة'), 1316), (('ال+', 'منصوص', 'على'), 1154), (('ال+', 'مشار', 'إلى'), 947), (('وفقا', 'ل+', 'ال+'), 906), (('منصوص', 'على', '+ها'), 869), (('على', '+ها', 'في'), 763), (('مشار', 'إلى', '+ها'), 724)]\n"
          ]
        }
      ],
      "source": [
        "fdist_dict = {}\n",
        "\n",
        "for index, row in concatenated_df.iterrows():\n",
        "    text = preprocess_text(row['text'])\n",
        "    label = row['labels']\n",
        "\n",
        "    if label not in fdist_dict:\n",
        "        fdist_dict[label] = FreqDist()\n",
        "\n",
        "    fdist_dict[label].update(ngrams(text, 3))\n",
        "\n",
        "for label, fdist in fdist_dict.items():\n",
        "    print(f\"Top 10 Trigrams for {label}:\")\n",
        "    print(fdist.most_common(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIfZKeBtsUjV",
        "outputId": "60be7d16-9a49-4788-ed16-d4e42ba155c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.649561053471668\n"
          ]
        }
      ],
      "source": [
        "pMT = fdist_dict.get('MT', FreqDist())\n",
        "pHT = fdist_dict.get('HT', FreqDist())\n",
        "\n",
        "def classify_sentence(test_sentence, pMT, pHT):\n",
        "    test_sentence = preprocess_text(test_sentence)\n",
        "\n",
        "    likelihood_HT = sum([math.log(pHT.get(trigram, 1e-10)) for trigram in ngrams(test_sentence, 3, pad_left=True, pad_right=True)])\n",
        "    likelihood_MT = sum([math.log(pMT.get(trigram, 1e-10)) for trigram in ngrams(test_sentence, 3, pad_left=True, pad_right=True)])\n",
        "    if likelihood_HT > likelihood_MT:\n",
        "        classification = \"HT\"\n",
        "    else:\n",
        "        classification = \"MT\"\n",
        "\n",
        "    return classification\n",
        "\n",
        "success_count = 0\n",
        "classifications = []\n",
        "\n",
        "for test_sentence, true_label in zip(test_df['text'], test_df['labels']):\n",
        "    classification = classify_sentence(test_sentence, pMT, pHT)\n",
        "    classifications.append(classification)  # Store classifications\n",
        "\n",
        "    if classification == true_label:\n",
        "        success_count += 1\n",
        "\n",
        "accuracy = success_count / len(test_df['text']) if len(test_df['text']) > 0 else 0\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "true_labels = test_df['labels'].tolist()\n",
        "predicted_labels = classifications\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "precision = precision_score(true_labels, predicted_labels, pos_label='HT')\n",
        "print(f\"Precision (HT): {precision}\")\n",
        "\n",
        "recall = recall_score(true_labels, predicted_labels, pos_label='HT')\n",
        "print(f\"Recall (HT): {recall}\")\n",
        "\n",
        "f1 = f1_score(true_labels, predicted_labels, pos_label='HT')\n",
        "print(f\"F1 Score (HT): {f1}\")\n"
      ],
      "metadata": {
        "id": "D3I4Ba2AHFLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d9b4a9-0685-4a8b-9f64-f577c5b57370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.649561053471668\n",
            "Precision (HT): 0.7660420215786485\n",
            "Recall (HT): 0.4306464485235435\n",
            "F1 Score (HT): 0.5513436190865434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "true_labels = test_df['labels'].tolist()\n",
        "predicted_labels = classifications\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "report = classification_report(true_labels, predicted_labels, target_names=['HT', 'MT'])\n",
        "print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YcnL0dVHxnC",
        "outputId": "8ac52ac0-4a46-49d7-f566-019f2a1dc1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.649561053471668\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "          HT       0.77      0.43      0.55      6265\n",
            "          MT       0.60      0.87      0.71      6265\n",
            "\n",
            "    accuracy                           0.65     12530\n",
            "   macro avg       0.69      0.65      0.63     12530\n",
            "weighted avg       0.69      0.65      0.63     12530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jl0OTEX9PxJ"
      },
      "outputs": [],
      "source": [
        "for prediction, true_label in zip(classifications, test_df['labels']):\n",
        "    print(f\"Prediction: {prediction}, True Label: {true_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch --set-upstream-to=origin/main\n",
        "!git config --global user.email \"humaid.alblooshi@mbzuai.ac.ae\"\n",
        "!git config --global user.name \"Humaid Alblooshi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRWNWiWrFPo_",
        "outputId": "a611a107-b282-473d-f993-330dece6825c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: the requested upstream branch 'origin/main' does not exist\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: If you are planning on basing your work on an upstream\u001b[m\n",
            "\u001b[33mhint: branch that already exists at the remote, you may need to\u001b[m\n",
            "\u001b[33mhint: run \"git fetch\" to retrieve it.\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: If you are planning to push out a new local branch that\u001b[m\n",
            "\u001b[33mhint: will track its remote counterpart, you may want to use\u001b[m\n",
            "\u001b[33mhint: \"git push -u\" to set the upstream config as you push.\u001b[m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/humaidalblooshi/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level.git\n",
        "\n",
        "%cd Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level\n",
        "\n",
        "!git config --global user.name \"Humaid Alblooshi\"\n",
        "!git config --global user.email \"humaid.alblooshi@mbzuai.ac.ae\"\n",
        "\n",
        "!git fetch\n",
        "\n",
        "!git branch --set-upstream-to=origin/main main\n",
        "\n",
        "!touch README.md\n",
        "!git add .\n",
        "!git commit -m \"Initial commit\"\n",
        "\n",
        "!git push origin HEAD:main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqOAIsLGILsc",
        "outputId": "b643967d-eae6-464c-89a7-0f5ad9cf92a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level'...\n",
            "warning: You appear to have cloned an empty repository.\n",
            "/content/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level\n",
            "fatal: branch 'main' does not exist\n",
            "[main (root-commit) dfcb351] Initial commit\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 README.md\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # !git clone https://github.com/humaidalblooshi/Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level.git\n",
        "# # %cd Automatic-detection-of-machine-translated-Arabic-Text-at-the-sentence-level\n",
        "!git add .\n",
        "!git commit -m \"First commit\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "id": "A-2mZdwDDv3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e0ed33-7e8b-49db-ec8e-0f8f9ac22179"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is based on 'origin/main', but the upstream is gone.\n",
            "  (use \"git branch --unset-upstream\" to fixup)\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBtqymYwFZ4M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}